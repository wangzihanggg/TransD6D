local_rank: 0
cls_id in lm_dataset.py 1
render data length:  0
Warning: Trainnig without rendered data will hurt model performance
Please generate rendered data from https://github.com/ethnhe/raster_triangle.
fused data length:  0
Warning: Trainnig without fused data will hurt model performance
Please generate fused data from https://github.com/ethnhe/raster_triangle.
train_dataset_size:  186
cls_id in lm_dataset.py 1
test_dataset_size:  1050
Number of model parameters:  37960145
local_rank: 0
/home/wangzihanggg/codespace/SwinDePose/swin_de_pose/mmsegmentation/mmseg/models/decode_heads/decode_head.py:94: UserWarning: For binary segmentation, we suggest using`out_channels = 1` to define the outputchannels of segmentor, and use `threshold`to convert seg_logist into a predictionapplying a threshold
  warnings.warn('For binary segmentation, we suggest using'
/home/wangzihanggg/codespace/SwinDePose/swin_de_pose/mmsegmentation/mmseg/models/losses/cross_entropy_loss.py:235: UserWarning: Default ``avg_non_ignore`` is False, if you would like to ignore the certain label and average loss over non-ignore labels, which is the same with PyTorch official cross_entropy, set ``avg_non_ignore=True``.
  warnings.warn(
/home/wangzihanggg/miniconda3/envs/pytorch/lib/python3.8/site-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.parallel.convert_syncbn_model is deprecated and will be removed by the end of February 2023. Use `torch.nn.SyncBatchNorm.convert_sync_batchnorm`.
  warnings.warn(msg, DeprecatedFeatureWarning)
Selected optimization level O0:  Pure FP32 training.
Defaults for this optimization level are:
enabled                : True
opt_level              : O0
cast_model_type        : torch.float32
patch_torch_functions  : False
keep_batchnorm_fp32    : None
master_weights         : False
loss_scale             : 1.0
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O0
cast_model_type        : torch.float32
patch_torch_functions  : False
keep_batchnorm_fp32    : None
master_weights         : False
loss_scale             : 1.0
==> Checkpoint 'train_log/test_1/ape/checkpoints/ape.pth.tar' not found
Totally train 2325 iters per gpu.
/home/wangzihanggg/miniconda3/envs/pytorch/lib/python3.8/site-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
  warnings.warn(msg, DeprecatedFeatureWarning)
/home/wangzihanggg/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.
  warnings.warn(warning.format(ret))
  0%|                                                                                                                                                                                               | 0/25 [00:00<?, ?it/s]
  0%|                                                                                                                                                                                               | 0/25 [00:02<?, ?it/s]
Traceback (most recent call last):
  File "apps/train_lm.py", line 750, in <module>
    train()
  File "apps/train_lm.py", line 732, in train
    trainer.train(
  File "apps/train_lm.py", line 513, in train
    _, loss, res = self.model_fn(self.model, batch, it=it)
  File "apps/train_lm.py", line 190, in model_fn
    end_points = model(cu_dt)
  File "/home/wangzihanggg/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/wangzihanggg/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1040, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/home/wangzihanggg/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1000, in _run_ddp_forward
    return module_to_run(*inputs[0], **kwargs[0])
  File "/home/wangzihanggg/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/wangzihanggg/miniconda3/envs/pytorch/lib/python3.8/site-packages/apex/amp/_initialize.py", line 198, in new_fwd
    output = old_fwd(*applier(args, input_caster),
  File "/home/wangzihanggg/codespace/SwinDePose/swin_de_pose/models/SwinDePose.py", line 257, in forward
    feat_final_nrm = intep(feat_up_nrm)
  File "/home/wangzihanggg/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/wangzihanggg/codespace/SwinDePose/swin_de_pose/mmsegmentation/mmseg/ops/wrappers.py", line 51, in forward
    return resize(x, size, None, self.mode, self.align_corners)
  File "/home/wangzihanggg/codespace/SwinDePose/swin_de_pose/mmsegmentation/mmseg/ops/wrappers.py", line 27, in resize
    return F.interpolate(input, size, scale_factor, mode, align_corners)
  File "/home/wangzihanggg/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py", line 3950, in interpolate
    return torch._C._nn.upsample_bilinear2d(input, output_size, align_corners, scale_factors)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 600.00 MiB (GPU 0; 5.81 GiB total capacity; 3.55 GiB already allocated; 487.19 MiB free; 3.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF